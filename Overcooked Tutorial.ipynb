{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36aba985",
   "metadata": {},
   "source": [
    "# Overcooked Tutorial\n",
    "This Notebook will demonstrate a couple of common use cases of the Overcooked-AI library, including loading and evaluating agents and visualizing trajectories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d85dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca4bad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 0.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 10/10 [00:00<00:00, 25.70it/s]\n",
      "Avg rew: 200.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 1/1 [00:00<00:00, 13.45it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc1eede49704795ad6101dc03d1aff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='timestep', max=399), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from overcooked_ai_py.agents.agent import AgentPair, RandomAgent\n",
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "\n",
    "# Here we create an evaluator for the cramped_room layout\n",
    "layout = \"cramped_room\"\n",
    "ae = AgentEvaluator.from_layout_name(mdp_params={\"layout_name\": layout, \"old_dynamics\": True}, \n",
    "                                     env_params={\"horizon\": 400})\n",
    "\n",
    "ap = AgentPair(RandomAgent(), RandomAgent())\n",
    "\n",
    "trajs = ae.evaluate_agent_pair(ap, 10)\n",
    "\n",
    "trajs2 = ae.evaluate_human_model_pair(1)\n",
    "\n",
    "\n",
    "StateVisualizer().display_rendered_trajectory(trajs2, ipython_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f70d1",
   "metadata": {},
   "source": [
    "# Deprecated stuff which requires BC and RL training (see README for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6b8ba",
   "metadata": {},
   "source": [
    "# Getting started: Training your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f96f8",
   "metadata": {},
   "source": [
    "You can train BC agents using files under the `human_aware_rl/imitation` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f493c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 15:27:21.010507: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 15:27:21.011163: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 15:27:21.013562: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 15:27:21.018862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743510441.028601  162213 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743510441.031721  162213 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743510441.038891  162213 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743510441.038907  162213 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743510441.038908  162213 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743510441.038908  162213 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-01 15:27:21.041349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /home/benjamin/Projects/overcooked_ai/src/human_aware_rl/static/human_data/cleaned/2019_hh_trials_train.pickle\n",
      "Number of trajectories processed for each layout: {'cramped_room': 14}\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 15:27:31.394058: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/benjamin/Projects/overcooked_ai/venv/lib/python3.10/site-packages/keras/src/models/functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: Overcooked_observation\n",
      "Received: inputs=['Tensor(shape=(None, 96))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446/446 - 1s - 3ms/step - loss: 0.9473 - sparse_categorical_accuracy: 0.7215 - val_loss: 0.8799 - val_sparse_categorical_accuracy: 0.7058 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "446/446 - 1s - 1ms/step - loss: 0.8432 - sparse_categorical_accuracy: 0.7245 - val_loss: 0.8174 - val_sparse_categorical_accuracy: 0.7056 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "446/446 - 1s - 1ms/step - loss: 0.8050 - sparse_categorical_accuracy: 0.7234 - val_loss: 0.7863 - val_sparse_categorical_accuracy: 0.7020 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "446/446 - 1s - 1ms/step - loss: 0.7885 - sparse_categorical_accuracy: 0.7234 - val_loss: 0.7751 - val_sparse_categorical_accuracy: 0.6980 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "446/446 - 1s - 1ms/step - loss: 0.7751 - sparse_categorical_accuracy: 0.7227 - val_loss: 0.7703 - val_sparse_categorical_accuracy: 0.7046 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "446/446 - 1s - 1ms/step - loss: 0.7693 - sparse_categorical_accuracy: 0.7242 - val_loss: 0.7671 - val_sparse_categorical_accuracy: 0.7064 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "446/446 - 1s - 1ms/step - loss: 0.7640 - sparse_categorical_accuracy: 0.7223 - val_loss: 0.7705 - val_sparse_categorical_accuracy: 0.7038 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "446/446 - 1s - 1ms/step - loss: 0.7594 - sparse_categorical_accuracy: 0.7238 - val_loss: 0.7643 - val_sparse_categorical_accuracy: 0.7078 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "446/446 - 1s - 1ms/step - loss: 0.7552 - sparse_categorical_accuracy: 0.7235 - val_loss: 0.7612 - val_sparse_categorical_accuracy: 0.7042 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "446/446 - 1s - 1ms/step - loss: 0.7500 - sparse_categorical_accuracy: 0.7229 - val_loss: 0.7644 - val_sparse_categorical_accuracy: 0.7080 - learning_rate: 0.0010\n",
      "Saving bc model at  tutorial_notebook_results/BC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Functional name=functional, built=True>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = \"cramped_room\" # any compatible layouts \n",
    "from human_aware_rl.imitation.behavior_cloning_tf2 import get_bc_params, train_bc_model\n",
    "from human_aware_rl.static import CLEAN_2019_HUMAN_DATA_TRAIN\n",
    "\n",
    "params_to_override = {\n",
    "    # this is the layouts where the training will happen\n",
    "    \"layouts\": [layout], \n",
    "    # this is the layout that the agents will be evaluated on\n",
    "    # Most of the time they should be the same, but because of refactoring some old layouts have more than one name and they need to be adjusted accordingly\n",
    "    \"layout_name\": layout, \n",
    "    \"data_path\": CLEAN_2019_HUMAN_DATA_TRAIN,\n",
    "    \"epochs\": 10,\n",
    "    \"old_dynamics\": True,\n",
    "}\n",
    "\n",
    "bc_params = get_bc_params(**params_to_override)\n",
    "train_bc_model(\"tutorial_notebook_results/BC\", bc_params, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc068ebc",
   "metadata": {},
   "source": [
    "# 1): Loading trained agents\n",
    "This section will show you how to load a pretrained agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a9df6",
   "metadata": {},
   "source": [
    "## 1.1) Loading BC agent\n",
    "The BC (behavior cloning) agents are trained separately without using Ray. We showed how to train a BC agent in the previous section, and to load a trained agent, we can use the load_bc_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94ab2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Functional name=functional, built=True>,\n",
       " {'eager': True,\n",
       "  'use_lstm': False,\n",
       "  'cell_size': 256,\n",
       "  'data_params': {'layouts': ['cramped_room'],\n",
       "   'check_trajectories': False,\n",
       "   'featurize_states': True,\n",
       "   'data_path': '/home/benjamin/Projects/overcooked_ai/src/human_aware_rl/static/human_data/cleaned/2019_hh_trials_train.pickle'},\n",
       "  'mdp_params': {'layout_name': 'cramped_room', 'old_dynamics': True},\n",
       "  'env_params': {'horizon': 400,\n",
       "   'mlam_params': {'start_orientations': False,\n",
       "    'wait_allowed': False,\n",
       "    'counter_goals': [],\n",
       "    'counter_drop': [],\n",
       "    'counter_pickup': [],\n",
       "    'same_motion_goals': True}},\n",
       "  'mdp_fn_params': {},\n",
       "  'mlp_params': {'num_layers': 2, 'net_arch': [64, 64]},\n",
       "  'training_params': {'epochs': 10,\n",
       "   'validation_split': 0.15,\n",
       "   'batch_size': 64,\n",
       "   'learning_rate': 0.001,\n",
       "   'use_class_weights': False},\n",
       "  'evaluation_params': {'ep_length': 400, 'num_games': 1, 'display': False},\n",
       "  'action_shape': (6,),\n",
       "  'observation_shape': (96,)})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import load_bc_model\n",
    "#this is the same path you used when training the BC agent\n",
    "bc_model_path = \"tutorial_notebook_results/BC\"\n",
    "bc_model, bc_params = load_bc_model(bc_model_path)\n",
    "bc_model, bc_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20526ac6",
   "metadata": {},
   "source": [
    "Now that we have loaded the model, since we used Tensorflow to train the agent, we need to wrap it so it is compatible with other agents. We can do it by converting it to a Rllib-compatible policy class, and wraps it as a RllibAgent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c37a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<human_aware_rl.rllib.rllib.RlLibAgent at 0x7ec356344760>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import _get_base_ae, BehaviorCloningPolicy\n",
    "bc_policy = BehaviorCloningPolicy.from_model(bc_model, bc_params, stochastic=True)\n",
    "# We need the featurization function that is specifically defined for BC agent\n",
    "# The easiest way to do it is to create a base environment from the configuration and extract the featurization function\n",
    "# The environment is also needed to do evaluation\n",
    "\n",
    "base_ae = _get_base_ae(bc_params)\n",
    "base_env = base_ae.env\n",
    "\n",
    "from human_aware_rl.rllib.rllib import RlLibAgent\n",
    "bc_agent0 = RlLibAgent(bc_policy, 0, base_env.featurize_state_mdp)\n",
    "bc_agent0\n",
    "\n",
    "bc_agent1 = RlLibAgent(bc_policy, 1, base_env.featurize_state_mdp)\n",
    "bc_agent1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c5687",
   "metadata": {},
   "source": [
    "Now we have a BC agent that is ready for evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73698e65",
   "metadata": {},
   "source": [
    "## 1.3) Loading & Creating Agent Pair\n",
    "\n",
    "To do evaluation, we need a pair of agents, or an AgentPair. We can directly load a pair of agents for evaluation, which we can do with the load_agent_pair function, or we can create an AgentPair manually from 2 separate RllibAgent instance. To directly load an AgentPair from a trainer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd83bc",
   "metadata": {},
   "source": [
    "To create an AgentPair manually, we can just pair together any 2 RllibAgent object. For example, we have created a **ppo_agent** and a **bc_agent**. To pair them up, we can just construct an AgentPair with them as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0acdeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.agent.AgentPair at 0x7ec35ada6110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.rllib.rllib import AgentPair\n",
    "ap_bc = AgentPair(bc_agent0, bc_agent1)\n",
    "ap_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6cafa",
   "metadata": {},
   "source": [
    "# 2): Evaluating AgentPair\n",
    "\n",
    "To evaluate an AgentPair, we need to first create an AgentEvaluator. You can create an AgentEvaluator in various ways, but the simpliest way to do so is from the layout_name. \n",
    "\n",
    "You can modify the settings of the layout by changing the **mdp_params** argument, but most of the time you should only need to include \"layout_name\", which is the layout you want to evaluate the agent pair on, and \"old_dynamics\", which determines whether the envrionment conforms to the design in the Neurips2019 paper, or whether the cooking should start automatically when all ingredients are present.  \n",
    "\n",
    "For the **env_params**, you can change how many steps are there in one evaluation. The default is 400, which means the game runs for 400 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95787dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.benchmarking.AgentEvaluator at 0x7ec356816950>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "# Here we create an evaluator for the cramped_room layout\n",
    "layout = \"cramped_room\"\n",
    "ae = AgentEvaluator.from_layout_name(mdp_params={\"layout_name\": layout, \"old_dynamics\": True}, \n",
    "                                     env_params={\"horizon\": 400})\n",
    "ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471aeda",
   "metadata": {},
   "source": [
    "To run evaluations, we can use the evaluate_agent_pair method associated with the AgentEvaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93676beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 50.00 (std: 20.49, se: 6.48); avg len: 400.00; : 100%|██████████| 10/10 [06:03<00:00, 36.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ep_dones': array([[False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True]], dtype=object),\n",
       " 'mdp_params': array([{'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]}],\n",
       "       dtype=object),\n",
       " 'ep_infos': array([[{'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.02959565, 0.00335462, 0.00523999, 0.01811266, 0.9391136 ,\n",
       "                 0.00458345]], dtype=float32)}, {'action_probs': array([[0.01633458, 0.00180742, 0.00459474, 0.00153629, 0.8903582 ,\n",
       "                 0.08536876]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02959565, 0.00335462, 0.00523999, 0.01811266, 0.9391136 ,\n",
       "                 0.00458345]], dtype=float32)}, {'action_probs': array([[0.01633458, 0.00180742, 0.00459474, 0.00153629, 0.8903582 ,\n",
       "                 0.08536876]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02959565, 0.00335462, 0.00523999, 0.01811266, 0.9391136 ,\n",
       "                 0.00458345]], dtype=float32)}, {'action_probs': array([[0.01633458, 0.00180742, 0.00459474, 0.00153629, 0.8903582 ,\n",
       "                 0.08536876]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[59, 98], [25, 77, 162, 176, 192, 239, 248, 263, 299, 322, 329]], 'useful_onion_pickup': [[59, 98], [25, 77, 162, 176, 192, 239, 248, 263, 299, 322, 329]], 'onion_drop': [[], []], 'useful_onion_drop': [[], []], 'potting_onion': [[93, 122], [66, 160, 171, 184, 230, 245, 257, 287, 311, 325]], 'dish_pickup': [[133, 192, 252, 300], []], 'useful_dish_pickup': [[133, 192, 252, 300], []], 'dish_drop': [[], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[152, 218, 277, 348], []], 'soup_delivery': [[157, 226, 283, 354], []], 'soup_drop': [[], []], 'optimal_onion_potting': [[93, 122], [66, 160, 171, 184, 230, 245, 257, 287, 311, 325]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[93, 122], [66, 160, 171, 184, 230, 245, 257, 287, 311, 325]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([80,  0]), 'cumulative_shaped_rewards_by_agent': array([38, 30])}, 'ep_sparse_r': 80, 'ep_shaped_r': 68, 'ep_sparse_r_by_agent': array([80,  0]), 'ep_shaped_r_by_agent': array([38, 30]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.00388156, 0.04580349, 0.00320465, 0.06328427, 0.8140892 ,\n",
       "                 0.06973678]], dtype=float32)}, {'action_probs': array([[0.00692742, 0.00300672, 0.00158265, 0.22232509, 0.7603754 ,\n",
       "                 0.00578274]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.00388156, 0.04580349, 0.00320465, 0.06328427, 0.8140892 ,\n",
       "                 0.06973678]], dtype=float32)}, {'action_probs': array([[0.00692742, 0.00300672, 0.00158265, 0.22232509, 0.7603754 ,\n",
       "                 0.00578274]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.00549334, 0.10048342, 0.00400928, 0.03858735, 0.723016  ,\n",
       "                 0.12841061]], dtype=float32)}, {'action_probs': array([[0.16571268, 0.00598366, 0.00466517, 0.03751759, 0.77806354,\n",
       "                 0.00805737]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[217, 239, 260, 301], [52, 70, 93, 100, 146, 165, 208, 296, 308, 379, 391]], 'useful_onion_pickup': [[217, 239, 260, 301], [52, 70, 93, 100, 146, 165, 208, 296, 308, 379, 391]], 'onion_drop': [[], []], 'useful_onion_drop': [[], []], 'potting_onion': [[224, 256, 284, 308], [62, 80, 96, 137, 161, 172, 212, 301, 376, 388]], 'dish_pickup': [[11, 150, 322], [233]], 'useful_dish_pickup': [[150, 322], [233]], 'dish_drop': [[], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[116, 193, 333], [276]], 'soup_delivery': [[123, 200, 348], [280]], 'soup_drop': [[], []], 'optimal_onion_potting': [[224, 256, 284, 308], [62, 80, 96, 137, 161, 172, 212, 301, 376, 388]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[224, 256, 284, 308], [62, 80, 96, 137, 161, 172, 212, 301, 376, 388]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([60, 20]), 'cumulative_shaped_rewards_by_agent': array([33, 38])}, 'ep_sparse_r': 80, 'ep_shaped_r': 71, 'ep_sparse_r_by_agent': array([60, 20]), 'ep_shaped_r_by_agent': array([33, 38]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.02008816, 0.00146393, 0.00356612, 0.00217856, 0.5967293 ,\n",
       "                 0.3759739 ]], dtype=float32)}, {'action_probs': array([[0.08834963, 0.01225818, 0.01160644, 0.16749813, 0.7117009 ,\n",
       "                 0.00858667]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [3, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.00928499, 0.01060397, 0.19483279, 0.0536701 , 0.6958521 ,\n",
       "                 0.03575607]], dtype=float32)}, {'action_probs': array([[0.0802537 , 0.01423095, 0.00898791, 0.22102208, 0.66971236,\n",
       "                 0.00579299]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.00928499, 0.01060397, 0.19483279, 0.0536701 , 0.6958521 ,\n",
       "                 0.03575607]], dtype=float32)}, {'action_probs': array([[0.0802537 , 0.01423095, 0.00898791, 0.22102208, 0.66971236,\n",
       "                 0.00579299]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[20, 129, 160, 183, 216, 341], [28, 50, 271, 311, 332]], 'useful_onion_pickup': [[20, 129, 160, 183, 216, 341], [28, 50, 271, 311, 332]], 'onion_drop': [[266], []], 'useful_onion_drop': [[], []], 'potting_onion': [[27, 151, 165, 199, 397], [39, 56, 302, 318, 353]], 'dish_pickup': [[61], [188, 360]], 'useful_dish_pickup': [[61], [188, 360]], 'dish_drop': [[], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[76], [219, 374]], 'soup_delivery': [[89], [395]], 'soup_drop': [[], [233]], 'optimal_onion_potting': [[27, 151, 165, 199, 397], [39, 56, 302, 318, 353]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[27, 151, 165, 199, 397], [39, 56, 302, 318, 353]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([20, 20]), 'cumulative_shaped_rewards_by_agent': array([23, 31])}, 'ep_sparse_r': 40, 'ep_shaped_r': 54, 'ep_sparse_r_by_agent': array([20, 20]), 'ep_shaped_r_by_agent': array([23, 31]), 'ep_length': 400}}],\n",
       "        ...,\n",
       "        [{'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.04935644, 0.00769745, 0.01943208, 0.14698276, 0.71112496,\n",
       "                 0.06540626]], dtype=float32)}, {'action_probs': array([[0.09921826, 0.03094076, 0.02958255, 0.04333772, 0.7233879 ,\n",
       "                 0.07353281]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.05138741, 0.00844179, 0.20402217, 0.00834253, 0.72185093,\n",
       "                 0.00595522]], dtype=float32)}, {'action_probs': array([[0.09041847, 0.07437126, 0.04407482, 0.03262845, 0.6628243 ,\n",
       "                 0.09568273]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.05138741, 0.00844179, 0.20402217, 0.00834253, 0.72185093,\n",
       "                 0.00595522]], dtype=float32)}, {'action_probs': array([[0.09041847, 0.07437126, 0.04407482, 0.03262845, 0.6628243 ,\n",
       "                 0.09568273]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[59, 353, 397], [16, 46, 59, 247, 285]], 'useful_onion_pickup': [[59, 353, 397], [16, 46, 59, 247, 285]], 'onion_drop': [[], []], 'useful_onion_drop': [[], []], 'potting_onion': [[187, 362], [39, 56, 71, 256, 290]], 'dish_pickup': [[211], [95, 305]], 'useful_dish_pickup': [[211], [95]], 'dish_drop': [[], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[312], [181]], 'soup_delivery': [[], [185]], 'soup_drop': [[323], []], 'optimal_onion_potting': [[187, 362], [39, 56, 71, 256, 290]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[187, 362], [39, 56, 71, 256, 290]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([ 0, 20]), 'cumulative_shaped_rewards_by_agent': array([14, 23])}, 'ep_sparse_r': 20, 'ep_shaped_r': 37, 'ep_sparse_r_by_agent': array([ 0, 20]), 'ep_shaped_r_by_agent': array([14, 23]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.0035689 , 0.0175731 , 0.00199272, 0.01730108, 0.94418526,\n",
       "                 0.0153789 ]], dtype=float32)}, {'action_probs': array([[0.00998235, 0.01201473, 0.01450587, 0.01273068, 0.82589144,\n",
       "                 0.12487496]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.0035689 , 0.0175731 , 0.00199272, 0.01730108, 0.94418526,\n",
       "                 0.0153789 ]], dtype=float32)}, {'action_probs': array([[0.00998235, 0.01201473, 0.01450587, 0.01273068, 0.82589144,\n",
       "                 0.12487496]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.0035689 , 0.0175731 , 0.00199272, 0.01730108, 0.94418526,\n",
       "                 0.0153789 ]], dtype=float32)}, {'action_probs': array([[0.00998235, 0.01201473, 0.01450587, 0.01273068, 0.82589144,\n",
       "                 0.12487496]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[22, 48, 172, 211, 318, 350], [36, 82, 176, 254, 343, 386]], 'useful_onion_pickup': [[22, 48, 172, 211, 318, 350], [36, 176, 254, 343]], 'onion_drop': [[336], []], 'useful_onion_drop': [[], []], 'potting_onion': [[43, 69, 206, 261], [56, 170, 182, 321, 357]], 'dish_pickup': [[118], [193]], 'useful_dish_pickup': [[118], [193]], 'dish_drop': [[], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[145], [226]], 'soup_delivery': [[153], [234]], 'soup_drop': [[], []], 'optimal_onion_potting': [[43, 69, 206, 261], [56, 170, 182, 321, 357]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[43, 69, 206, 261], [56, 170, 182, 321, 357]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([20, 20]), 'cumulative_shaped_rewards_by_agent': array([20, 23])}, 'ep_sparse_r': 40, 'ep_shaped_r': 43, 'ep_sparse_r_by_agent': array([20, 20]), 'ep_shaped_r_by_agent': array([20, 23]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02568045, 0.00244885, 0.0059708 , 0.01021909, 0.95157915,\n",
       "                 0.00410169]], dtype=float32)}, {'action_probs': array([[1.25656482e-02, 1.10673124e-03, 1.12981675e-02, 4.17099660e-03,\n",
       "                 9.70197380e-01, 6.61083788e-04]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.01634446, 0.00112028, 0.02648092, 0.00417192, 0.92300946,\n",
       "                 0.02887294]], dtype=float32)}, {'action_probs': array([[2.1594845e-02, 6.6315318e-03, 2.1733539e-03, 4.2444453e-04,\n",
       "                 9.6782124e-01, 1.3546443e-03]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.02116827, 0.00556038, 0.04709384, 0.00676879, 0.87770957,\n",
       "                 0.04169921]], dtype=float32)}, {'action_probs': array([[0.07467106, 0.01782728, 0.01128339, 0.01266938, 0.8398957 ,\n",
       "                 0.0436532 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.03352775, 0.00517877, 0.07638063, 0.00460425, 0.814604  ,\n",
       "                 0.06570455]], dtype=float32)}, {'action_probs': array([[0.0609338 , 0.04411375, 0.18160708, 0.08741237, 0.6204387 ,\n",
       "                 0.00549429]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[23, 60, 168, 182, 243, 347, 364, 381], [25, 48, 253, 294, 311]], 'useful_onion_pickup': [[23, 168, 182, 243, 347, 364, 381], [25, 48, 253, 294, 311]], 'onion_drop': [[], []], 'useful_onion_drop': [[], []], 'potting_onion': [[33, 164, 171, 193, 268, 360, 378], [43, 52, 281, 299, 342]], 'dish_pickup': [[299], [60, 188, 357]], 'useful_dish_pickup': [[299], [60, 188, 357]], 'dish_drop': [[], []], 'useful_dish_drop': [[], []], 'soup_pickup': [[326], [156, 221]], 'soup_delivery': [[335], [163, 233]], 'soup_drop': [[], []], 'optimal_onion_potting': [[33, 164, 171, 193, 268, 360, 378], [43, 52, 281, 299, 342]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[33, 164, 171, 193, 268, 360, 378], [43, 52, 281, 299, 342]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([20, 40]), 'cumulative_shaped_rewards_by_agent': array([29, 34])}, 'ep_sparse_r': 60, 'ep_shaped_r': 63, 'ep_sparse_r_by_agent': array([20, 40]), 'ep_shaped_r_by_agent': array([29, 34]), 'ep_length': 400}}]],\n",
       "       dtype=object),\n",
       " 'ep_states': array([[<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3dc9c8610>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3568c6cb0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec35635bfd0>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355de3880>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec35612dd50>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3561c2770>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3561db5e0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec35612f910>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec35612cd90>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355a9d750>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355a41f90>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355914dc0>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355a73b20>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec356815ab0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec356386ef0>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355a437c0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355a41180>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355a431f0>],\n",
       "        ...,\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355661270>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3556616c0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec35521f790>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355790370>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355b62110>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3559081f0>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3558059c0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355cdd5a0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355807c70>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec35502a4d0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3550b8af0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3550d7160>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3550fd000>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355505420>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec355505990>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3559d6dd0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3550663b0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ec3551a7790>]],\n",
       "       dtype=object),\n",
       " 'ep_returns': array([80, 80, 40, 20, 60, 40, 60, 20, 40, 60]),\n",
       " 'metadatas': {},\n",
       " 'ep_rewards': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=object),\n",
       " 'ep_lengths': array([400, 400, 400, 400, 400, 400, 400, 400, 400, 400]),\n",
       " 'ep_actions': array([[((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0)), ...,\n",
       "         ((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0))],\n",
       "        [((0, 0), (0, 0)), ((0, 0), (0, 0)), ((-1, 0), (0, -1)), ...,\n",
       "         ((0, 0), (0, 0)), ((0, 0), (-1, 0)), ((0, 0), (0, 0))],\n",
       "        [((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0)), ...,\n",
       "         ('interact', (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0))],\n",
       "        ...,\n",
       "        [((0, 0), (0, 0)), ((0, 0), (0, 0)), ((-1, 0), (-1, 0)), ...,\n",
       "         ('interact', (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0))],\n",
       "        [((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0)), ...,\n",
       "         ((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0))],\n",
       "        [((0, 0), (0, 0)), ((0, 0), (0, 0)), ((0, 0), (0, 0)), ...,\n",
       "         ((0, 0), (0, 0)), ((0, 0), (-1, 0)), ((0, 0), (0, 0))]],\n",
       "       dtype=object),\n",
       " 'env_params': array([{'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1}],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ap: The AgentPair we created earlier\n",
    "# 10: how many times we should run the evaluation since the policy is stochastic\n",
    "trajs = ae.evaluate_agent_pair(ap_bc, 10)\n",
    "trajs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b6cca",
   "metadata": {},
   "source": [
    "The result returned by the AgentEvaluator contains detailed information about the evaluation runs, including actions taken by each agent at each timestep. Usually you don't need to directly interact with them, but the most direct performance measures can be retrieved with result[\"ep_returns\"], which returns the average sparse reward of each evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fed7df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80, 80, 40, 20, 60, 40, 60, 20, 40, 60])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajs[\"ep_returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48875a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 60.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 1/1 [00:36<00:00, 36.53s/it]\n"
     ]
    }
   ],
   "source": [
    "result = ae.evaluate_agent_pair(ap_bc, 1, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898bae8",
   "metadata": {},
   "source": [
    "# 3): Visualization\n",
    "\n",
    "We can also visualize the trajectories of agents. One way is to run the web demo with the agents you choose, and the specific instructions can be found in the [overcooked_demo](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/overcooked_demo) module, which requires some setup. Another simpler way is to use the StateVisualizer, which uses the information returned by the AgentEvaluator to create a simple dynamic visualization. You can checkout [this Colab Notebook](https://colab.research.google.com/drive/1AAVP2P-QQhbx6WTOnIG54NXLXFbO7y6n#scrollTo=6Xlu54MkiXCR) that let you play with fixed agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "464d0c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979fb004f0fb432685e055e590fdbaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='timestep', max=399), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "StateVisualizer().display_rendered_trajectory(trajs, ipython_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b62122",
   "metadata": {},
   "source": [
    "This should spawn a window where you can see what the agents are doing at each timestep. You can drag the slider to go forward and backward in time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
